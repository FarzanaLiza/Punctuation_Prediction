{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3482cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60241ad7",
   "metadata": {},
   "source": [
    "# TED Talk en data\n",
    "\n",
    ">Data source: TED Talks Transcripts for NLP<br>\n",
    "https://www.kaggle.com/datasets/miguelcorraljr/ted-ultimate-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978cf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data in the directory as a csv file\n",
    "\n",
    "import pandas as pd\n",
    "ted_data = pd.read_csv('./ted_talks_en.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66dc79b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Check data type\n",
    "ted_data.head()\n",
    "ted_data.shape\n",
    "print(type(ted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd5a70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>all_speakers</th>\n",
       "      <th>occupations</th>\n",
       "      <th>about_speakers</th>\n",
       "      <th>views</th>\n",
       "      <th>recorded_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>event</th>\n",
       "      <th>native_lang</th>\n",
       "      <th>available_lang</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>{0: 'Al Gore'}</td>\n",
       "      <td>{0: ['climate advocate']}</td>\n",
       "      <td>{0: 'Nobel Laureate Al Gore focused the world’...</td>\n",
       "      <td>3523392</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...</td>\n",
       "      <td>272.0</td>\n",
       "      <td>977</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>{243: 'New thinking on the climate crisis', 54...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_averting_the...</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>{0: 'Hans Rosling'}</td>\n",
       "      <td>{0: ['global health expert; data visionary']}</td>\n",
       "      <td>{0: 'In Hans Rosling’s hands, data sings. Glob...</td>\n",
       "      <td>14501685</td>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1190</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>{2056: \"Own your body's data\", 2296: 'A visual...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_the_bes...</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talk_id                            title     speaker_1  \\\n",
       "0        1      Averting the climate crisis       Al Gore   \n",
       "1       92  The best stats you've ever seen  Hans Rosling   \n",
       "\n",
       "          all_speakers                                    occupations  \\\n",
       "0       {0: 'Al Gore'}                      {0: ['climate advocate']}   \n",
       "1  {0: 'Hans Rosling'}  {0: ['global health expert; data visionary']}   \n",
       "\n",
       "                                      about_speakers     views recorded_date  \\\n",
       "0  {0: 'Nobel Laureate Al Gore focused the world’...   3523392    2006-02-25   \n",
       "1  {0: 'In Hans Rosling’s hands, data sings. Glob...  14501685    2006-02-22   \n",
       "\n",
       "  published_date    event native_lang  \\\n",
       "0     2006-06-27  TED2006          en   \n",
       "1     2006-06-27  TED2006          en   \n",
       "\n",
       "                                      available_lang  comments  duration  \\\n",
       "0  ['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...     272.0       977   \n",
       "1  ['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...     628.0      1190   \n",
       "\n",
       "                                              topics  \\\n",
       "0  ['alternative energy', 'cars', 'climate change...   \n",
       "1  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  {243: 'New thinking on the climate crisis', 54...   \n",
       "1  {2056: \"Own your body's data\", 2296: 'A visual...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ted.com/talks/al_gore_averting_the...   \n",
       "1  https://www.ted.com/talks/hans_rosling_the_bes...   \n",
       "\n",
       "                                         description  \\\n",
       "0  With the same humor and humanity he exuded in ...   \n",
       "1  You've never seen data presented like this. Wi...   \n",
       "\n",
       "                                          transcript  \n",
       "0  Thank you so much, Chris. And it's truly a gre...  \n",
       "1  About 10 years ago, I took on the task to teac...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1313b1",
   "metadata": {},
   "source": [
    "# Processing TED Talk en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3f764d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packeages\n",
    "\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610e2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the 'transcript' feature of each presenter.\n",
    "# Each row represents each presenter\n",
    "\n",
    "ted_data_list = ted_data['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17ca44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ted_data_list))\n",
    "print(type(ted_data_list[5]))\n",
    "#print(ted_data_list[0]) # One presenters' speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36aaa806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3338\n"
     ]
    }
   ],
   "source": [
    "print(len(ted_data_list))\n",
    "#print(ted_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9cc28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/farzanayasmin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required packages\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Sentence tokenizer package for Natural Language Toolkit (NLTK) library\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8133d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length for is larger (548 > 512), so we need to divide the paragraphs with less than 512 words.\n",
    "# Dividing the 'ted_data_list' data with ten sentences per paragraph so that they don't exceed more than 510 words for each paragraph\n",
    "# And keep the dataset as pandas.core.series.Series with strings\n",
    "\n",
    "def chunk_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Create chunks of ten sentences each\n",
    "    chunks = [' '.join(sentences[i:i+10]) for i in range(0, len(sentences), 6)]\n",
    "    return chunks\n",
    "\n",
    "# Apply the function\n",
    "chunks_list = ted_data_list.apply(chunk_sentences).tolist()\n",
    "\n",
    "# Flatten the list of lists\n",
    "resulting_data = [chunk for sublist in chunks_list for chunk in sublist]\n",
    "\n",
    "# Convert the list back to a Series\n",
    "resulting_series = pd.Series(resulting_data)\n",
    "\n",
    "#print(resulting_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "805a618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. And I say that sincerely, partly because (Mock sob) I need that. (Laughter) Put yourselves in my position. (Laughter) I flew on Air Force Two for eight years. (Laughter) Now I have to take off my shoes or boots to get on an airplane! (Laughter) (Applause) I'll tell you one quick story to illustrate what that's been like for me. (Laughter) It's a true story — every bit of this is true. Soon after Tipper and I left the — (Mock sob) White House — (Laughter) we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville.\n"
     ]
    }
   ],
   "source": [
    "print(resulting_series[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2743d183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(resulting_series[0]))\n",
    "print(type(resulting_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb9606f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the 'wikitext-103-v1' processsed data has space before each punctuation, we create our data the same.\n",
    "# Adding a space before puctuations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def add_space_before_punctuation(text):\n",
    "    punctuation_marks = set(\".,;:!?-'()\")\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if char in punctuation_marks and (not result or result[-1] != ' '):\n",
    "            result.append(' ')\n",
    "        result.append(char)\n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "# Apply the function to each element in the Series\n",
    "processed_data_series = resulting_series.apply(add_space_before_punctuation)\n",
    "\n",
    "# The result is a new Series with the processed strings\n",
    "#print(processed_data_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6447c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n",
      "Thank you so much , Chris . And it 's truly a great honor to have the opportunity to come to this stage twice ; I 'm extremely grateful . I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night . And I say that sincerely , partly because (Mock sob ) I need that . (Laughter ) Put yourselves in my position . (Laughter ) I flew on Air Force Two for eight years . (Laughter ) Now I have to take off my shoes or boots to get on an airplane ! (Laughter ) (Applause ) I 'll tell you one quick story to illustrate what that 's been like for me . (Laughter ) It 's a true story — every bit of this is true . Soon after Tipper and I left the — (Mock sob ) White House — (Laughter ) we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville .\n"
     ]
    }
   ],
   "source": [
    "print(type(processed_data_series))\n",
    "print(type(processed_data_series[0]))\n",
    "print(processed_data_series[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a1ad857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a60bee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load package\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9647cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# First, convert the series to a dataframe with column name as 'text'\n",
    "df = processed_data_series.to_frame(name='text')\n",
    "\n",
    "# Convert the dataframe to a Dataset\n",
    "ted_data_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# To verify the conversion\n",
    "print(type(ted_data_dataset))\n",
    "\n",
    "# Now, you can use save_dataset(ds)\n",
    "# save_dataset(ted_data_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11caaee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(ted_data_dataset[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fede5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"There was no motorcade back there . (Laughter ) You 've heard of phantom limb pain ? (Laughter ) This was a rented Ford Taurus . (Laughter ) It was dinnertime , and we started looking for a place to eat . We were on I -40 . We got to Exit 238 , Lebanon , Tennessee . We got off the exit , we found a Shoney 's restaurant . Low -cost family restaurant chain , for those of you who don 't know it . We went in and sat down at the booth , and the waitress came over , made a big commotion over Tipper . (Laughter ) She took our order , and then went to the couple in the booth next to us , and she lowered her voice so much , I had to really strain to hear what she was saying .\"}\n"
     ]
    }
   ],
   "source": [
    "print(ted_data_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb79ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special labels for predicting punctuations\n",
    "\n",
    "special_labels = {',': 'I-COMMA',\n",
    "                  '.': 'I-DOT',\n",
    "                  '?': 'I-QMARK',\n",
    "                  '!': 'I-EMARK',\n",
    "                  ':': 'I-COLON',\n",
    "                  ';': 'I-SEMICOLON',\n",
    "                  '-': 'I-HYPHEN', # Label for the hyphen (new)\n",
    "                  \"'\": 'I-APOST'  } # Label for the apostrophe (new)\n",
    "normal_label = 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310d700",
   "metadata": {},
   "source": [
    "### Filtering out paragraph that are less than 10 and more than 510 tokens or words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a64c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descrete_and_label(list_of_lines):\n",
    "    list_of_lists = []\n",
    "    for i, line in enumerate(list_of_lines):\n",
    "        tkn_line = tokenizer.tokenize(line)\n",
    "        if len(tkn_line) < 10 or len(tkn_line) > 510:\n",
    "            continue\n",
    "        for word in line.split():\n",
    "            lbl = normal_label\n",
    "            brek = False\n",
    "            sl = special_labels.get(word, None)\n",
    "            if sl:\n",
    "                if list_of_lists:\n",
    "                    list_of_lists[-1][2] = sl\n",
    "                    brek = True\n",
    "            if not brek:\n",
    "                list_of_lists.append([i, word, lbl])\n",
    "    return list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41eca1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_dataset(ds, path):\n",
    "    filtered = []\n",
    "    filtered += [i['text'] for i in ds if len(i['text']) > 20]\n",
    "    dataset_1 = descrete_and_label(filtered)\n",
    "    train_data = pd.DataFrame(dataset_1, columns=[\"sentence_id\", \"words\", \"labels\"])\n",
    "    train_data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "238b0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(dataset_1)\n",
    "#train_data.shape\n",
    "#print(dataset_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2859820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder called processed_tedtalk in the working directory\n",
    "\n",
    "import os\n",
    "\n",
    "directory = 'processed_tedtalk'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1783411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ReadInstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a4f0c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "7025\n",
      "10 20\n",
      "7025\n",
      "20 30\n",
      "7025\n",
      "30 40\n",
      "7025\n",
      "40 50\n",
      "7025\n",
      "50 60\n",
      "7024\n",
      "60 70\n",
      "7024\n",
      "70 80\n",
      "7024\n",
      "80 90\n",
      "7024\n",
      "90 100\n",
      "7024\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Our ted_data_dataset is already loaded\n",
    "\n",
    "binz = 10\n",
    "for i in range(binz):\n",
    "    start_pct = int(i * (100/binz))\n",
    "    end_pct = int((i+1) * (100/binz))\n",
    "    print(start_pct, end_pct)\n",
    "\n",
    "    # Using the shard method to split ted_data_dataset\n",
    "    sub_dataset = ted_data_dataset.shard(num_shards=binz, index=i)\n",
    "    \n",
    "    print(len(sub_dataset))\n",
    "    save_dataset(sub_dataset, f'./processed_tedtalk/train{i}-{binz}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d814b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'But what we really need is , of course , a search function , a search function where we can copy the data up to a searchable format and get it out in the world . And what do we hear when we go around ? I \\'ve done anthropology on the main statistical units . Everyone says , \"It \\'s impossible . This can \\'t be done . Our information is so peculiar in detail , so that cannot be searched as others can be searched . We cannot give the data free to the students , free to the entrepreneurs of the world .\" But this is what we would like to see , isn \\'t it ? The publicly funded data is down here . And we would like flowers to grow out on the net .'}\n"
     ]
    }
   ],
   "source": [
    "type(sub_dataset[5])\n",
    "print(sub_dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c107be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ec376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff508b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
